# -*- coding: utf-8 -*-
"""Semi_supervised_nnunet_AbdominalCT_segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mR6SpC5d-15_OIQWu3pQWQWxrMdMdzn1
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
kulkerjun_abdominalct_path = kagglehub.dataset_download('kulkerjun/abdominalct')

print('Data source import complete.')

import os
import sys
import math
import random
import time
import json
import glob
import csv
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict

import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast, GradScaler
from torch.utils.data import Dataset, DataLoader
from torchvision.utils import save_image

# ---- Albumentations ----
try:
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    _HAVE_ALBU = True
except Exception:
    _HAVE_ALBU = False
    print("[WARN] Albumentations not found; using minimal fallback transforms.", file=sys.stderr)

# =====================================================
# Config
# =====================================================

@dataclass
class Config:
    # Paths
    images_root: str = "/kaggle/input/abdominalct/AbdominalCT/images"
    masks_root:  str = "/kaggle/input/abdominalct/AbdominalCT/masks"
    work_dir:    str = "/kaggle/working/abdominalct_nnunet2d"

    # Training
    seed: int = 42
    img_size: int = 512
    batch_size: int = 8
    num_workers: int = 2
    epochs: int = 10
    lr: float = 3e-4
    weight_decay: float = 1e-4
    amp: bool = True

    # Model
    in_channels: int = 1
    base_channels: int = 32          # nnUNet starts at 32
    deep_supervision: bool = True     # enable DS heads
    use_bn: bool = True               # BatchNorm (else InstanceNorm)

    # Task
    num_classes: int = 5              # background + 4 organs

    # Loss
    dice_ce_lambda_dice: float = 1.0
    dice_ce_lambda_ce:   float = 1.0
    class_weights_ce: Optional[List[float]] = None

    # Data
    val_split: float = 0.1
    drop_empty_masks: bool = True

    # Augmentations
    aug_strength: str = "light"

    # Semi-supervised (Mean Teacher)
    use_semi: bool = True
    unlabeled_batch_size: int = 8
    ema_decay: float = 0.99
    unsup_weight: float = 1.0
    unsup_rampup_epochs: int = 10
    pseudo_conf_thresh: float = 0.6

    # Logging / saving
    save_every_epochs: int = 5
    samples_every_n_steps: int = 1000

CFG = Config()

# =====================================================
# Utils
# =====================================================

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        try:
            torch.cuda.manual_seed_all(seed)
        except RuntimeError as e:
            print("[WARN] CUDA seed failed (device assert previously?):", e)
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False


def ensure_dir(p):
    os.makedirs(p, exist_ok=True)


def list_pngs(root: str) -> List[str]:
    return sorted(glob.glob(os.path.join(root, "**", "*.png"), recursive=True))


def pair_image_mask(images_root: str, masks_root: str) -> List[Tuple[str, str]]:
    imgs = list_pngs(images_root)
    mask_rel_set = {os.path.relpath(p, masks_root) for p in list_pngs(masks_root)}
    pairs = []
    for ip in imgs:
        rel = os.path.relpath(ip, images_root)
        if rel in mask_rel_set:
            pairs.append((ip, os.path.join(masks_root, rel)))
    return pairs


def list_unlabeled_images(images_root: str, masks_root: str) -> List[str]:
    imgs = list_pngs(images_root)
    mask_rel_set = {os.path.relpath(p, masks_root) for p in list_pngs(masks_root)}
    return [ip for ip in imgs if os.path.relpath(ip, images_root) not in mask_rel_set]


def is_mask_nonempty(mask_path: str, max_class: int = 4) -> bool:
    m = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
    if m is None:
        return False
    if m.ndim == 3:
        m = cv2.cvtColor(m, cv2.COLOR_BGR2GRAY)
    m = np.where(m == 255, 0, m)
    return np.any(m > 0)


def apply_clahe(image_uint8: np.ndarray) -> np.ndarray:
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    return clahe.apply(image_uint8)


def normalize_zscore(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:
    m = float(x.mean()); s = float(x.std())
    return (x - m) / (s + eps) if s >= eps else (x - m)

# =====================================================
# Datasets
# =====================================================

class AbdominalCT2DDataset(Dataset):
    def __init__(self, pairs: List[Tuple[str, str]], img_size=512, augment=True, aug_strength="medium"):
        self.pairs = pairs
        self.img_size = img_size
        self.augment = augment
        self.aug_strength = aug_strength
        self.transform = self._build_transforms()
    def _build_transforms(self):
        size = self.img_size
        if _HAVE_ALBU:
            aug = [
                A.LongestMaxSize(max_size=size),
                A.PadIfNeeded(min_height=size, min_width=size, border_mode=cv2.BORDER_CONSTANT, border_value=0, mask_value=0),
            ]
            if self.augment:
                if self.aug_strength in ("medium", "heavy"):
                    aug += [
                        A.HorizontalFlip(p=0.5),
                        A.VerticalFlip(p=0.2),
                        A.Affine(translate_percent=0.0625, scale=(0.9, 1.1), rotate=(-15, 15),
                                 mode=cv2.BORDER_CONSTANT, cval=0, cval_mask=0, p=0.7),
                    ]
                if self.aug_strength == "heavy":
                    aug += [
                        A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),
                        A.ElasticTransform(alpha=20, sigma=5, alpha_affine=10,
                                           border_mode=cv2.BORDER_CONSTANT, cval=0, cval_mask=0, p=0.2),
                    ]
            aug += [ToTensorV2(transpose_mask=True)]
            return A.Compose(aug)
        return None
    def __len__(self):
        return len(self.pairs)
    def __getitem__(self, idx):
        ip, mp = self.pairs[idx]
        img = cv2.imread(ip, cv2.IMREAD_GRAYSCALE); assert img is not None, f"img read fail: {ip}"
        if img.dtype != np.uint8: img = img.astype(np.uint8)
        img = apply_clahe(img)
        img = img.astype(np.float32)/255.0
        img = normalize_zscore(img).astype(np.float32)
        m = cv2.imread(mp, cv2.IMREAD_UNCHANGED); assert m is not None, f"mask read fail: {mp}"
        if m.ndim == 3: m = cv2.cvtColor(m, cv2.COLOR_BGR2GRAY)
        m = np.where(m == 255, 0, m).astype(np.int64)
        m = np.clip(m, 0, 4)
        if _HAVE_ALBU:
            t = self.transform(image=img, mask=m)
            return t["image"].float(), t["mask"].long(), os.path.basename(ip)
        # fallback
        img_r = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)
        m_r   = cv2.resize(m,   (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)
        return torch.from_numpy(img_r).unsqueeze(0).float(), torch.from_numpy(m_r).long(), os.path.basename(ip)

class UnlabeledCT2DDataset(Dataset):
    def __init__(self, img_paths: List[str], img_size=512, aug_strength="medium"):
        self.img_paths = img_paths
        self.img_size = img_size
        self.aug_strength = aug_strength
        self.transform = self._build_transforms()
    def _build_transforms(self):
        size = self.img_size
        if _HAVE_ALBU:
            aug = [
                A.LongestMaxSize(max_size=size),
                A.PadIfNeeded(min_height=size, min_width=size, border_mode=cv2.BORDER_CONSTANT, border_value=0, mask_value=0),
            ]
            if self.aug_strength in ("medium", "heavy"):
                aug += [
                    A.HorizontalFlip(p=0.5),
                    A.Affine(translate_percent=0.0625, scale=(0.9, 1.1), rotate=(-15, 15),
                             mode=cv2.BORDER_CONSTANT, cval=0, cval_mask=0, p=0.7),
                ]
            if self.aug_strength == "heavy":
                aug += [A.GaussNoise(var_limit=(5.0, 25.0), p=0.2)]
            aug += [ToTensorV2(transpose_mask=True)]
            return A.Compose(aug)
        return None
    def __len__(self):
        return len(self.img_paths)
    def __getitem__(self, idx):
        ip = self.img_paths[idx]
        img = cv2.imread(ip, cv2.IMREAD_GRAYSCALE); assert img is not None, f"img read fail: {ip}"
        if img.dtype != np.uint8: img = img.astype(np.uint8)
        img = apply_clahe(img)
        img = img.astype(np.float32)/255.0
        img = normalize_zscore(img).astype(np.float32)
        if _HAVE_ALBU:
            v1 = self.transform(image=img)["image"].float()
            v2 = self.transform(image=img)["image"].float()
        else:
            img_r = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)
            v1 = torch.from_numpy(img_r).unsqueeze(0).float()
            v2 = torch.from_numpy(img_r).unsqueeze(0).float()
        return v1, v2, os.path.basename(ip)

# =====================================================
# nnU-Netâ€“style 2D model
# =====================================================

class ConvDropNormNonlin(nn.Module):
    def __init__(self, in_ch, out_ch, norm="bn", p_drop=0.0):
        super().__init__()
        bias = False
        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=bias)
        if norm == "bn":
            self.norm = nn.BatchNorm2d(out_ch)
        else:
            self.norm = nn.InstanceNorm2d(out_ch, affine=True)
        self.nonlin = nn.LeakyReLU(0.01, inplace=True)
        self.drop = nn.Dropout2d(p_drop) if p_drop > 0 else nn.Identity()
    def forward(self, x):
        return self.drop(self.nonlin(self.norm(self.conv(x))))

class StackedConvBlocks(nn.Module):
    def __init__(self, in_ch, out_ch, n_conv=2, norm="bn"):
        super().__init__()
        blocks = []
        c_in = in_ch
        for i in range(n_conv):
            c_out = out_ch
            blocks.append(ConvDropNormNonlin(c_in, c_out, norm=norm))
            c_in = c_out
        self.blocks = nn.Sequential(*blocks)
    def forward(self, x):
        return self.blocks(x)

class Encoder(nn.Module):
    def __init__(self, in_ch, base=32, levels=5, norm="bn"):
        super().__init__()
        chs = [base * (2**i) for i in range(levels)]
        self.stems = nn.ModuleList()
        self.pools = nn.ModuleList()
        c_in = in_ch
        for c in chs:
            self.stems.append(StackedConvBlocks(c_in, c, n_conv=2, norm=norm))
            self.pools.append(nn.MaxPool2d(2))
            c_in = c
        self.chs = chs
    def forward(self, x):
        feats = []
        out = x
        for stem, pool in zip(self.stems, self.pools):
            out = stem(out)
            feats.append(out)
            out = pool(out)
        return feats, out  # feats at each scale, and bottom input

class Bottleneck(nn.Module):
    def __init__(self, in_ch, out_ch, norm="bn"):
        super().__init__()
        self.block = StackedConvBlocks(in_ch, out_ch, n_conv=2, norm=norm)
    def forward(self, x):
        return self.block(x)

class Decoder(nn.Module):
    def __init__(self, chs: List[int], norm="bn", deep_supervision=True, n_classes=5):
        super().__init__()
        # chs: encoder channels per stage (lowest to highest: [..., 256, 128, 64, 32])
        self.deep_supervision = deep_supervision
        ups, decs = [], []
        ds_heads = []
        for i in range(len(chs)-1, 0, -1):
            up_in = chs[i]
            up_out = chs[i-1]
            ups.append(nn.ConvTranspose2d(up_in, up_out, 2, stride=2))
            decs.append(StackedConvBlocks(up_out*2, up_out, n_conv=2, norm=norm))
            if self.deep_supervision and i <= 2:  # ds for two highest-resolution decoder outputs
                ds_heads.append(nn.Conv2d(up_out, n_classes, 1))
            else:
                ds_heads.append(None)
        self.ups = nn.ModuleList(ups)
        self.decs = nn.ModuleList(decs)
        self.ds_heads = nn.ModuleList([h if h is not None else nn.Identity() for h in ds_heads])

    def forward(self, feats: List[torch.Tensor], bottom: torch.Tensor):
        # feats: list from encoder (low->high resolution)
        x = bottom
        ds_outputs = []
        idx_ds = 0
        for i in range(len(self.ups)):
            x = self.ups[i](x)
            enc_feat = feats[-(i+1)]  # from high-res towards low-res
            x = self.decs[i](torch.cat([x, enc_feat], dim=1))
            # collect deep supervision at highest two decoder scales
            if self.deep_supervision and (len(self.ups) - i) <= 2:
                ds_outputs.append(self.ds_heads[idx_ds](x))
                idx_ds += 1
            else:
                idx_ds += 1
        return x, ds_outputs  # main head input, list of DS logits (coarser to finer order)

class NNUNet2D(nn.Module):
    def __init__(self, in_channels=1, num_classes=5, base=32, levels=5, norm="bn", deep_supervision=True):
        super().__init__()
        self.deep_supervision = deep_supervision
        self.encoder = Encoder(in_channels, base=base, levels=levels, norm=norm)
        enc_chs = self.encoder.chs  # [32,64,128,256,512]
        self.bottleneck = Bottleneck(enc_chs[-1], enc_chs[-1]*2, norm=norm)  # 512->1024
        # decoder expects channel list including bottleneck out followed by encoder chs
        dec_chs = enc_chs + [enc_chs[-1]*2]
        self.decoder = Decoder(dec_chs, norm=norm, deep_supervision=deep_supervision, n_classes=num_classes)
        self.out_head = nn.Conv2d(enc_chs[0], num_classes, 1)
    def forward(self, x):
        feats, bottom_in = self.encoder(x)
        bott = self.bottleneck(bottom_in)
        dec_out, ds_logits = self.decoder(feats, bott)
        out = self.out_head(dec_out)
        if self.deep_supervision and self.training and len(ds_logits) > 0:
            return [out] + ds_logits[::-1]  # return from finest to coarser heads
        return out

# =====================================================
# Losses & Metrics
# =====================================================

class SoftDiceLoss(nn.Module):
    def __init__(self, smooth: float = 1.0, include_background: bool = True):
        super().__init__()
        self.smooth = smooth
        self.include_background = include_background
    def forward(self, logits, target):
        C = logits.shape[1]
        target_1h = F.one_hot(target, num_classes=C).permute(0,3,1,2).float()
        probs = torch.softmax(logits, dim=1)
        dims = (0,2,3)
        inter = torch.sum(probs * target_1h, dims)
        cardinal = torch.sum(probs + target_1h, dims)
        dice_c = (2.0*inter + self.smooth) / (cardinal + self.smooth)
        if not self.include_background and dice_c.shape[0] > 1:
            dice_c = dice_c[:,1:]
        return 1.0 - dice_c.mean()

class DiceCELoss(nn.Module):
    def __init__(self, lambda_dice=1.0, lambda_ce=1.0, ce_weights=None, include_background=True):
        super().__init__()
        self.dice = SoftDiceLoss(include_background=include_background)
        self.ce_weights = torch.tensor(ce_weights).float() if ce_weights is not None else None
        self.ld = lambda_dice
        self.lc = lambda_ce
    def forward(self, logits, target):
        ce = F.cross_entropy(logits, target, weight=(self.ce_weights.to(logits.device) if self.ce_weights is not None else None))
        dice = self.dice(logits, target)
        return self.ld * dice + self.lc * ce

@torch.no_grad()
def compute_iou_dice(logits, target, num_classes) -> Dict[str, float]:
    pred = torch.argmax(torch.softmax(logits, dim=1), dim=1)
    ious, dices, per_class = [], [], {}
    for c in range(num_classes):
        pc = (pred == c); tc = (target == c)
        inter = (pc & tc).sum().float(); union = (pc | tc).sum().float()
        dice = (2*inter) / (pc.sum() + tc.sum() + 1e-6); iou = inter / (union + 1e-6)
        ious.append(iou); dices.append(dice); per_class[c] = {"iou": iou.item(), "dice": dice.item()}
    return {"miou": torch.stack(ious).mean().item(), "mdice": torch.stack(dices).mean().item(), "per_class": per_class}

# =====================================================
# Training helpers
# =====================================================

def ema_update(student: nn.Module, teacher: nn.Module, decay: float):
    with torch.no_grad():
        for ps, pt in zip(student.parameters(), teacher.parameters()):
            pt.data.mul_((decay)).add_(ps.data, alpha=(1.0 - decay))

def sigmoid_rampup(current, rampup_length):
    if rampup_length == 0:
        return 1.0
    current = np.clip(current, 0.0, rampup_length)
    phase = 1.0 - current / rampup_length
    return float(math.exp(-5.0 * phase * phase))

def consistency_loss(student_logits, teacher_logits, threshold: float):
    ps = torch.softmax(student_logits, dim=1)
    pt = torch.softmax(teacher_logits.detach(), dim=1)
    conf, _ = pt.max(dim=1, keepdim=True)
    mask = (conf >= threshold).float()
    return ((ps - pt) ** 2 * mask).mean()

# =====================================================
# Train / Validate
# =====================================================

def save_sample_grid(x, y, logits, out_dir, step_or_epoch, prefix):
    with torch.no_grad():
        probs = torch.softmax(logits, dim=1)
        pred = torch.argmax(probs, dim=1, keepdim=True).float()
        x_vis = (x - x.min()) / (x.max() - x.min() + 1e-6)
        y_vis = y.unsqueeze(1).float() / max(1, logits.shape[1]-1)
        grid = torch.cat([x_vis[:4], y_vis[:4], pred[:4]], dim=0)
        p = os.path.join(out_dir, f"{prefix}_samples_{step_or_epoch:07d}.png")
        save_image(grid, p, nrow=4)
        print(f"[Saved samples] {p}")


def train_one_epoch(model, loader, optimizer, scaler, loss_fn, device, epoch, cfg: Config, out_dir,
                    unl_loader=None, teacher_model: Optional[nn.Module]=None):
    model.train()
    if teacher_model is not None:
        teacher_model.eval()
    t0 = time.time()
    running_sup = 0.0
    running_unsup = 0.0
    steps = 0
    unl_iter = iter(unl_loader) if unl_loader is not None else None

    for x, y, _ in loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        u1 = u2 = None
        if unl_iter is not None:
            try:
                u1, u2, _ = next(unl_iter)
            except StopIteration:
                unl_iter = iter(unl_loader)
                u1, u2, _ = next(unl_iter)
            u1 = u1.to(device, non_blocking=True)
            u2 = u2.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)
        if cfg.amp:
            with autocast(device_type="cuda", dtype=torch.float16):
                logits_list = model(x)
                if isinstance(logits_list, list):
                    # deep supervision: weight coarser heads less
                    weights = [1.0, 0.5, 0.25][:len(logits_list)]
                    sup = 0.0
                    for w, lg in zip(weights, logits_list):
                        if lg.shape[-2:] != y.shape[-2:]:
                            lg = F.interpolate(lg, size=y.shape[-2:], mode="bilinear", align_corners=False)
                        sup = sup + w * loss_fn(lg, y)
                else:
                    sup = loss_fn(logits_list, y)
                total = sup
                if teacher_model is not None and u1 is not None:
                    stu_logits = model(u1)
                    if isinstance(stu_logits, list):
                        stu_logits = stu_logits[0]
                    with torch.no_grad():
                        tea_logits = teacher_model(u2)
                        if isinstance(tea_logits, list):
                            tea_logits = tea_logits[0]
                    w = cfg.unsup_weight * sigmoid_rampup(epoch, cfg.unsup_rampup_epochs)
                    cons = consistency_loss(stu_logits, tea_logits, cfg.pseudo_conf_thresh)
                    total = total + w * cons
            scaler.scale(total).backward(); scaler.step(optimizer); scaler.update()
        else:
            logits_list = model(x)
            if isinstance(logits_list, list):
                    # deep supervision: weight coarser heads less
                    weights = [1.0, 0.5, 0.25][:len(logits_list)]
                    sup = 0.0
                    for w, lg in zip(weights, logits_list):
                        if lg.shape[-2:] != y.shape[-2:]:
                            lg = F.interpolate(lg, size=y.shape[-2:], mode="bilinear", align_corners=False)
                        sup = sup + w * loss_fn(lg, y)
            else:
                sup = loss_fn(logits_list, y)
            total = sup
            if teacher_model is not None and u1 is not None:
                stu_logits = model(u1)
                if isinstance(stu_logits, list): stu_logits = stu_logits[0]
                with torch.no_grad():
                    tea_logits = teacher_model(u2)
                    if isinstance(tea_logits, list): tea_logits = tea_logits[0]
                w = cfg.unsup_weight * sigmoid_rampup(epoch, cfg.unsup_rampup_epochs)
                cons = consistency_loss(stu_logits, tea_logits, cfg.pseudo_conf_thresh)
                total = total + w * cons
            total.backward(); optimizer.step()

        if teacher_model is not None:
            ema_update(model, teacher_model, cfg.ema_decay)

        running_sup += float(sup.item())
        if teacher_model is not None and u1 is not None:
            running_unsup += float(cons.item())
        steps += 1
        if steps % max(1, cfg.samples_every_n_steps) == 0:
            main_logits = logits_list[0] if isinstance(logits_list, list) else logits_list
            save_sample_grid(x, y, main_logits, out_dir, epoch*100000+steps, prefix="train")

    return {"sup_loss": running_sup/max(1,steps), "unsup_loss": (running_unsup/max(1,steps) if teacher_model is not None else 0.0), "time": time.time()-t0}

@torch.no_grad()
def validate(model, loader, loss_fn, device, epoch, cfg: Config, out_dir, num_classes: int):
    model.eval()
    losses, mm = [], []
    first = True
    for x, y, _ in loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        if cfg.amp:
            with autocast(device_type="cuda", dtype=torch.float16):
                logits_list = model(x)
                logits = logits_list[0] if isinstance(logits_list, list) else logits_list
                if logits.shape[-2:] != y.shape[-2:]:
                    logits = F.interpolate(logits, size=y.shape[-2:], mode="bilinear", align_corners=False)
                loss = loss_fn(logits, y)
        else:
            logits_list = model(x)
            logits = logits_list[0] if isinstance(logits_list, list) else logits_list
            if logits.shape[-2:] != y.shape[-2:]:
                logits = F.interpolate(logits, size=y.shape[-2:], mode="bilinear", align_corners=False)
            loss = loss_fn(logits, y)
        losses.append(float(loss.item()))
        mm.append(compute_iou_dice(logits, y, num_classes))
        if first:
            save_sample_grid(x, y, logits, out_dir, epoch, prefix="val"); first = False
    avg_loss = float(np.mean(losses)) if losses else 0.0
    mdice = float(np.mean([m["mdice"] for m in mm])) if mm else 0.0
    miou  = float(np.mean([m["miou"] for m in mm])) if mm else 0.0
    per_class_accum: Dict[int, Dict[str, List[float]]] = {}
    for m in mm:
        for c, d in m["per_class"].items():
            per_class_accum.setdefault(c, {"iou": [], "dice": []})
            per_class_accum[c]["iou"].append(d["iou"])
            per_class_accum[c]["dice"].append(d["dice"])
    per_class = {c: {"iou": float(np.mean(v["iou"])) if v["iou"] else 0.0,
                     "dice": float(np.mean(v["dice"])) if v["dice"] else 0.0}
                 for c, v in per_class_accum.items()}
    return avg_loss, mdice, miou, per_class

# =====================================================
# CSV logging
# =====================================================

def write_history_csv(history, csv_path: str, num_classes: int):
    header = ["epoch", "train_loss", "train_unsup", "val_loss", "mdice", "miou"]
    for c in range(num_classes):
        header += [f"class{c}_dice", f"class{c}_iou"]
    rows = []
    for e in history:
        row = [e.get("epoch"), e.get("train_loss"), e.get("train_unsup", 0.0), e.get("val_loss"), e.get("mdice"), e.get("miou")]
        pc = e.get("per_class", {})
        for c in range(num_classes):
            m = pc.get(c, {"dice": None, "iou": None}); row += [m.get("dice"), m.get("iou")]
        rows.append(row)
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    with open(csv_path, "w", newline="") as f:
        writer = csv.writer(f); writer.writerow(header); writer.writerows(rows)

# =====================================================
# Main
# =====================================================

def build_dataloaders(pairs: List[Tuple[str, str]], cfg: Config):
    random.shuffle(pairs)
    n = len(pairs); n_val = max(1, int(n * cfg.val_split))
    val_pairs = pairs[:n_val]; train_pairs = pairs[n_val:]
    ds_train = AbdominalCT2DDataset(train_pairs, img_size=cfg.img_size, augment=True, aug_strength=cfg.aug_strength)
    ds_val   = AbdominalCT2DDataset(val_pairs,   img_size=cfg.img_size, augment=False, aug_strength=cfg.aug_strength)
    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True,
                          num_workers=cfg.num_workers, pin_memory=torch.cuda.is_available(), drop_last=True)
    dl_val   = DataLoader(ds_val, batch_size=max(1, cfg.batch_size//2), shuffle=False,
                          num_workers=cfg.num_workers, pin_memory=torch.cuda.is_available())
    # unlabeled
    unl_dl, unl_imgs = None, []
    if cfg.use_semi:
        unl_imgs = list_unlabeled_images(cfg.images_root, cfg.masks_root)
        if len(unl_imgs) > 0:
            ds_unl = UnlabeledCT2DDataset(unl_imgs, img_size=cfg.img_size, aug_strength=cfg.aug_strength)
            unl_dl = DataLoader(ds_unl, batch_size=cfg.unlabeled_batch_size, shuffle=True,
                                num_workers=cfg.num_workers, pin_memory=torch.cuda.is_available(), drop_last=True)
            print(f"Unlabeled images: {len(unl_imgs)} (EMA teacher consistency enabled)")
        else:
            print("No unlabeled images found; training fully supervised.")
    return dl_train, dl_val, train_pairs, val_pairs, unl_dl, unl_imgs


def main(cfg: Config = CFG):
    os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "1")

    # Env overrides (e.g. CFG_EPOCHS=10)
    for field in list(vars(cfg).keys()):
        env_key = f"CFG_{field}".upper()
        if env_key in os.environ:
            try:
                setattr(cfg, field, eval(os.environ[env_key]))
            except Exception:
                setattr(cfg, field, os.environ[env_key])

    set_seed(cfg.seed)
    ensure_dir(cfg.work_dir)

    pairs = pair_image_mask(cfg.images_root, cfg.masks_root)
    if len(pairs) == 0:
        raise RuntimeError("No (image, mask) pairs found. Check dataset paths and file names.")
    print(f"Found {len(pairs)} labeled pairs. num_classes={cfg.num_classes}")

    # quick sanity of labels (first 300 masks)
    weird = set()
    for _, mp in pairs[:300]:
        m = cv2.imread(mp, cv2.IMREAD_UNCHANGED)
        if m is None: continue
        if m.ndim == 3: m = cv2.cvtColor(m, cv2.COLOR_BGR2GRAY)
        u = np.unique(m)
        weird.update([v for v in u if v < 0 or v > (cfg.num_classes-1)])
    if weird:
        print(f"[WARN] out-of-range mask labels encountered (will be clamped in loader): {sorted(list(weird))}")

    if cfg.drop_empty_masks:
        b = len(pairs)
        pairs = [(ip, mp) for (ip, mp) in pairs if is_mask_nonempty(mp, max_class=cfg.num_classes-1)]
        a = len(pairs)
        print(f"Filtered empty masks: kept {a}/{b} ({(a/max(1,b))*100:.1f}%)")

    dl_train, dl_val, train_pairs, val_pairs, unl_dl, unl_imgs = build_dataloaders(pairs, cfg)
    print(f"Train: {len(train_pairs)} | Val: {len(val_pairs)}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    norm = "bn" if cfg.use_bn else "in"
    model = NNUNet2D(in_channels=cfg.in_channels, num_classes=cfg.num_classes,
                     base=cfg.base_channels, levels=5, norm=norm, deep_supervision=cfg.deep_supervision).to(device)

    teacher_model = None
    if cfg.use_semi:
        teacher_model = NNUNet2D(in_channels=cfg.in_channels, num_classes=cfg.num_classes,
                                 base=cfg.base_channels, levels=5, norm=norm, deep_supervision=cfg.deep_supervision).to(device)
        teacher_model.load_state_dict(model.state_dict())
        for p in teacher_model.parameters():
            p.requires_grad_(False)

    loss_fn = DiceCELoss(lambda_dice=cfg.dice_ce_lambda_dice, lambda_ce=cfg.dice_ce_lambda_ce,
                         ce_weights=cfg.class_weights_ce, include_background=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    scaler = GradScaler("cuda", enabled=(cfg.amp and torch.cuda.is_available()))

    best_dice = -1.0
    history = []

    for epoch in range(1, cfg.epochs + 1):
        print(f"\nEpoch {epoch}/{cfg.epochs}")
        tr = train_one_epoch(model, dl_train, optimizer, scaler, loss_fn, device, epoch, cfg, cfg.work_dir,
                             unl_loader=unl_dl, teacher_model=teacher_model)
        va_loss, mdice, miou, per_class = validate(model, dl_val, loss_fn, device, epoch, cfg, cfg.work_dir, cfg.num_classes)
        print(f"  train_sup_loss={tr['sup_loss']:.4f}  train_unsup_loss={tr['unsup_loss']:.4f}  (time {tr['time']:.1f}s)")
        print(f"  val_loss={va_loss:.4f}  mdice={mdice:.4f}  miou={miou:.4f}")
        print(f"  per-class: {json.dumps(per_class)}")

        history.append({"epoch": epoch, "train_loss": tr['sup_loss'], "train_unsup": tr['unsup_loss'],
                        "val_loss": va_loss, "mdice": mdice, "miou": miou, "per_class": per_class})
        with open(os.path.join(cfg.work_dir, "history.json"), "w") as f:
            json.dump(history, f, indent=2)
        write_history_csv(history, os.path.join(cfg.work_dir, "history.csv"), cfg.num_classes)

        if mdice > best_dice:
            best_dice = mdice
            best_path = os.path.join(cfg.work_dir, f"best_nnunet2d_mdice{best_dice:.4f}_epoch{epoch}.pt")
            torch.save({"epoch": epoch, "model": model.state_dict(),
                        "teacher": (teacher_model.state_dict() if teacher_model is not None else None),
                        "cfg": vars(cfg)}, best_path)
            print(f"  [BEST] Saved {best_path}")
        if epoch % cfg.save_every_epochs == 0:
            ckpt_path = os.path.join(cfg.work_dir, f"ckpt_epoch{epoch}.pt")
            torch.save({"epoch": epoch, "model": model.state_dict(),
                        "teacher": (teacher_model.state_dict() if teacher_model is not None else None),
                        "cfg": vars(cfg)}, ckpt_path)
            print(f"  [CKPT] Saved {ckpt_path}")

    print("\nTraining complete. Best val Dice:", best_dice)

if __name__ == "__main__":
    print("Using configuration:")
    print(json.dumps(vars(CFG), indent=2))
    main(CFG)